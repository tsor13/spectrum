# uv run launch_configs/launch_sbatch_grid.py launch_configs/distributional.yaml --max-concurrent 8
command: "uv run src/spectrum/distributional_alignment/eval_distributional.py --log_wandb"

# Sbatch configuration for SLURM job submission
sbatch:
  partition: gpu-a100                   # partition
  account: xlab                         # account
  time: "36:00:00"                      # wall time
  ntasks: 1                             # number of tasks
  mem: 80G                              # memory per node
  gpus-per-task: 1                      # GPUs per task
  cpus-per-task: 4                      # CPUs per task
  chdir: "/gscratch/xlab/tsor13/spectrum-staging"  # working directory
  output: "logs/%x-%j.out"              # output file pattern
  job-name: "dist-eval"                   # base job name (will be appended with index)

base:                                         # merged into every run
  batch_size: 16
  max_eval: 1000
  random_seed: 42

runs:                                         # model variants
  - model_name: google/gemma-3-12b-it
    format: chat

  - model_name: google/gemma-3-12b-pt
    format: colon

  - model_name: qwen/Qwen3-14B
    format: chat

  - model_name: qwen/Qwen3-14B-Base
    format: colon
  
  - model_name: meta-llama/Llama-3.1-8B-Instruct
    format: chat

  - model_name: meta-llama/Llama-3.1-8B
    format: colon
  
  - model_name: tsor13/spectrum-Qwen3-14B-v0
    format: spectrum  

  - model_name: tsor13/spectrum-Llama-3.1-8B-v0
    format: spectrum

  - model_name: tsor13/spectrum-gemma-3-12b-v0
    format: spectrum


datasets:
  - task: global_oqa
  - task: nytimes
  - task: mpi
  - task: rotten_tomatoes
  - task: urn
  - task: habermas
  - task: numbergame